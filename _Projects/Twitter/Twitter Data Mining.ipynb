{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll go over the following topics:\n",
    "- Collecting data from Twitter\n",
    "- Text pre-processing using NLTK\n",
    "- Analysing term frequencies\n",
    "- Data Visualization\n",
    "- Sentiment Analysis\n",
    "\n",
    "This project was made by going through this (great) blog [post](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/).  Code extracts are mainly taken from there. I've also added pieces of code at various places following my interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to authorise our app to access Twitter on our behalf, we need to use the OAuth interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "import os\n",
    "consumer_key = os.environ['CONSUMER_KEY']\n",
    "consumer_secret = os.environ['CONSUMER_SECRET']\n",
    "access_token = os.environ['ACCESS_TOKEN']\n",
    "access_secret = os.environ['ACCESS_SECRET']\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display of the 10 tweets of the home timeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @sebastien_abis: Tr√®s heureux d'√™tre l'invit√© du mois des entretiens #Euromed #IHEDN, le 12 √† #Marseille et le 13 √† #Paris, pour parler‚Ä¶\n",
      "Just got off a call to thank folks who are working hard to help more Americans across the country sign up for healt‚Ä¶ https://t.co/A9QvCySWJ8\n",
      "Arabie saoudite : les paris risqu√©s de Mohammed ben Salmane \n",
      "Le point de vue Francis Perrin, directeur de recherche‚Ä¶ https://t.co/RoiSUyEsQf\n",
      "RT @IRIS_SUP_: F√©licitation aux nouveaux dipl√¥m√©s IRIS Sup' 2017! üëè\n",
      "Pr√®s de 500 personnes √©taient pr√©sentes, dipl√¥m√©s, parents, amis, tous‚Ä¶\n",
      "RT @carole_gomez: Conf√©rence ce soir sur lutte contre la manipulation des comp√©titions en Fr. √† @InstitutIRIS avec T.Pujol, C.Kalb @ffhandb‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(5):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display of the JSON response :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Mon Dec 11 21:13:03 +0000 2017\", \"id\": 940328598958936065, \"id_str\": \"940328598958936065\", \"text\": \"RT @sebastien_abis: Tr\\u00e8s heureux d'\\u00eatre l'invit\\u00e9 du mois des entretiens #Euromed #IHEDN, le 12 \\u00e0 #Marseille et le 13 \\u00e0 #Paris, pour parler\\u2026\", \"truncated\": false, \"entities\": {\"hashtags\": [{\"text\": \"Euromed\", \"indices\": [72, 80]}, {\"text\": \"IHEDN\", \"indices\": [81, 87]}, {\"text\": \"Marseille\", \"indices\": [97, 107]}, {\"text\": \"Paris\", \"indices\": [119, 125]}], \"symbols\": [], \"user_mentions\": [{\"screen_name\": \"sebastien_abis\", \"name\": \"S\\u00e9bastien ABIS\", \"id\": 622745686, \"id_str\": \"622745686\", \"indices\": [3, 18]}], \"urls\": []}, \"source\": \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"user\": {\"id\": 265897069, \"id_str\": \"265897069\", \"name\": \"IRIS\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "    \n",
    "for status in tweepy.Cursor(api.home_timeline).items(1):\n",
    "    # Process a single status\n",
    "    print(json.dumps(status._json)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing connections :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 265897069, \"id_str\": \"265897069\", \"name\": \"IRIS\", \"screen_name\": \"InstitutIRIS\", \"location\": \"Paris\", \"description\": \"Acteur fran\\u00e7ais de la recherche strat\\u00e9gique et g\\u00e9opolitique. Ses activit\\u00e9s : la recherche, l\\u2019organisation de manifestations, la publication, la formation.\", \"url\": \"http://t.co/EctAbFIhS6\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/EctAbFIhS6\", \"expanded_url\": \"http://iris-france.org\", \"display_url\": \"iris-france.org\", \"indices\": [0, 22]}]}, \"description\": {\"urls\": []}}, \"protected\": false, \"followers_count\": 24432, \"friends_count\": 422, \"listed_count\": 603, \"created_at\": \"Mon Mar 14 09:29:08 +0000 2011\", \"favourites_count\": 670, \"utc_offset\": 3600, \"time_zone\": \"Paris\", \"geo_enabled\": true, \"verified\": false, \"statuses_count\": 9310, \"lang\": \"fr\", \"status\": {\"created_at\": \"Mon Dec 11 21:13:03 +0000 2017\", \"id\": 940328598958936065, \"id_str\": \"940328598958936065\", \"text\": \"RT @sebastien_abis: Tr\\u00e8s heureux d'\\u00eatre l'invit\\u\n"
     ]
    }
   ],
   "source": [
    "for friend in tweepy.Cursor(api.friends).items(1):\n",
    "    print(json.dumps(friend._json)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing my own tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"¬´¬†Cool, bi√®res et bonbons gratuits¬†!¬†¬ª¬†: plong√©e dans une start-up d√©jant√©e\" #actualites #feedly https://t.co/cKWCAPZSVr\n",
      "Sun Jul 24 17:30:30 +0000 2016\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline).items():\n",
    "    #print(json.dumps(tweet._json))\n",
    "    print(tweet._json['text'])\n",
    "    print(tweet._json['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables :\n",
    "- `text`: the text of the tweet itself\n",
    "- `created_at`: the date of creation\n",
    "- `favorite_count`, `retweet_count`: the number of favourites and retweets\n",
    "- `favorited`, `retweeted`: boolean stating whether the authenticated user (you) have favourited or retweeted this tweet\n",
    "- `lang`: acronym for the language (e.g. ‚Äúen‚Äù for english)\n",
    "- `id`: the tweet identifier\n",
    "- `place`, `coordinates`, `geo`: geo-location information if available\n",
    "- `user`: the author‚Äôs full profile\n",
    "- `entities`: list of entities like URLs, @-mentions, hashtags and symbols\n",
    "- `in_reply_to_user_id`: user identifier if the tweet is a reply to a specific user\n",
    "- `in_reply_to_status_id`: status identifier id the tweet is a reply to a specific status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing tweets from another user :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just got off a call to thank folks who are working hard to help more Americans across the country sign up for healt‚Ä¶ https://t.co/A9QvCySWJ8\n",
      "Mon Dec 11 20:16:29 +0000 2017\n",
      "RT @ObamaFoundation: Watch: We hosted a Town Hall in New Delhi with @BarackObama and young leaders about how to drive change and make an im‚Ä¶\n",
      "Mon Dec 04 22:57:47 +0000 2017\n",
      "Michelle and I are delighted to congratulate Prince Harry and Meghan Markle on their engagement. We wish you a life‚Ä¶ https://t.co/KC9nmjZPuX\n",
      "Mon Nov 27 21:13:50 +0000 2017\n",
      "From the Obama family to yours, we wish you a Happy Thanksgiving full of joy and gratitude. https://t.co/xAvSQwjQkz\n",
      "Thu Nov 23 14:44:27 +0000 2017\n",
      "ME:  Joe, about halfway through the speech, I‚Äôm gonna wish you a happy birth--\n",
      "BIDEN:  IT‚ÄôS MY BIRTHDAY!\n",
      "ME:  Joe.‚Ä¶ https://t.co/5qLUsDoaMi\n",
      "Mon Nov 20 19:02:11 +0000 2017\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(5):\n",
    "    #print(json.dumps(tweet._json))\n",
    "    print(tweet._json['text'])\n",
    "    print(tweet._json['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text tokenization using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'got', 'off', 'a', 'call', 'to', 'thank', 'folks', 'who', 'are', 'working', 'hard', 'to', 'help', 'more', 'Americans', 'across', 'the', 'country', 'sign', 'up', 'for', 'healt‚Ä¶', 'https', ':', '//t.co/A9QvCySWJ8']\n",
      "['RT', '@', 'ObamaFoundation', ':', 'Watch', ':', 'We', 'hosted', 'a', 'Town', 'Hall', 'in', 'New', 'Delhi', 'with', '@', 'BarackObama', 'and', 'young', 'leaders', 'about', 'how', 'to', 'drive', 'change', 'and', 'make', 'an', 'im‚Ä¶']\n",
      "['Michelle', 'and', 'I', 'are', 'delighted', 'to', 'congratulate', 'Prince', 'Harry', 'and', 'Meghan', 'Markle', 'on', 'their', 'engagement', '.', 'We', 'wish', 'you', 'a', 'life‚Ä¶', 'https', ':', '//t.co/KC9nmjZPuX']\n",
      "['From', 'the', 'Obama', 'family', 'to', 'yours', ',', 'we', 'wish', 'you', 'a', 'Happy', 'Thanksgiving', 'full', 'of', 'joy', 'and', 'gratitude', '.', 'https', ':', '//t.co/xAvSQwjQkz']\n",
      "['ME', ':', 'Joe', ',', 'about', 'halfway', 'through', 'the', 'speech', ',', 'I', '‚Äô', 'm', 'gon', 'na', 'wish', 'you', 'a', 'happy', 'birth', '--', 'BIDEN', ':', 'IT', '‚Äô', 'S', 'MY', 'BIRTHDAY', '!', 'ME', ':', 'Joe.‚Ä¶', 'https', ':', '//t.co/5qLUsDoaMi']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(5):\n",
    "    tweet_text = tweet._json['text']\n",
    "    print(word_tokenize(tweet_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Enhancing the tokenization by accounting for @-mentions, emoticons, URLs and hash-tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'got', 'off', 'a', 'call', 'to', 'thank', 'folks', 'who', 'are', 'working', 'hard', 'to', 'help', 'more', 'Americans', 'across', 'the', 'country', 'sign', 'up', 'for', 'healt', '‚Ä¶', 'https://t.co/A9QvCySWJ8']\n",
      "['RT', '@ObamaFoundation', ':', 'Watch', ':', 'We', 'hosted', 'a', 'Town', 'Hall', 'in', 'New', 'Delhi', 'with', '@BarackObama', 'and', 'young', 'leaders', 'about', 'how', 'to', 'drive', 'change', 'and', 'make', 'an', 'im', '‚Ä¶']\n",
      "['Michelle', 'and', 'I', 'are', 'delighted', 'to', 'congratulate', 'Prince', 'Harry', 'and', 'Meghan', 'Markle', 'on', 'their', 'engagement', '.', 'We', 'wish', 'you', 'a', 'life', '‚Ä¶', 'https://t.co/KC9nmjZPuX']\n",
      "['From', 'the', 'Obama', 'family', 'to', 'yours', ',', 'we', 'wish', 'you', 'a', 'Happy', 'Thanksgiving', 'full', 'of', 'joy', 'and', 'gratitude', '.', 'https://t.co/xAvSQwjQkz']\n",
      "['ME', ':', 'Joe', ',', 'about', 'halfway', 'through', 'the', 'speech', ',', 'I', '‚Äô', 'm', 'gonna', 'wish', 'you', 'a', 'happy', 'birth', '-', '-', 'BIDEN', ':', 'IT', '‚Äô', 'S', 'MY', 'BIRTHDAY', '!', 'ME', ':', 'Joe', '.', '‚Ä¶', 'https://t.co/5qLUsDoaMi']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(5):\n",
    "    tweet_text = tweet._json['text']\n",
    "    print(preprocess(tweet_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the blog post, the tokeniser is probably far from perfect, but it gives you the general idea. The tokenisation is based on regular expressions (regexp), which is a common choice for this type of problem. Some particular types of tokens (e.g. phone numbers or chemical names) will not be captured, and will be probably broken into several tokens. To overcome this problem, as well as to improve the richness of your pre-processing pipeline, you can improve the regular expressions, or even employ more sophisticated techniques like Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the terms used in the last 200 tweets of Barack Obama :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 234), ('the', 139), ('to', 111), (':', 99), (',', 94)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "count_all = Counter()\n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(200):\n",
    "    tweet_text = tweet._json['text']\n",
    "    # Create a list with all the terms\n",
    "    terms_all = [term for term in preprocess(tweet_text)]\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all)\n",
    "    \n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most frequent words are not exactly meaningful.  Let's remove the common words, called \"stop-words\".  We can use NLTK for this.  We also include tweet-specific stop-words such as RT (used for re-tweets), \"via\" and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     d:\\Profiles\\cnozaradan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'RT', '‚Ä¶', '‚Äô', '‚Äú', 'The']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now adapt the code.  Let's embed it in a method.  We also seperatly count hashtag terms and sequences of two terms (makes easier to identify the topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_stats(twitter_id, tweet_nbr=300, most_common=5, lang='english'):\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop = stopwords.words(lang) + punctuation + ['rt','via','RT','‚Ä¶','‚Äô','‚Äú','The','√©','les','a','La','Le','Il','√™tre','‚Üí']\n",
    "    count_all = Counter()\n",
    "    count_single = Counter()\n",
    "    count_hash = Counter()\n",
    "    count_bigram = Counter()\n",
    "    for tweet in tweepy.Cursor(api.user_timeline, id=twitter_id).items(tweet_nbr):\n",
    "        tweet_text = tweet._json['text']\n",
    "        # Create a list with all the terms\n",
    "        terms_preprocessed = preprocess(tweet_text)\n",
    "        terms_all = [term for term in terms_preprocessed if term not in stop]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "        # Count terms only once, equivalent to Document Frequency\n",
    "        terms_single = set(terms_all)\n",
    "        count_single.update(terms_single)\n",
    "        # Count hashtags only\n",
    "        terms_hash = [term for term in terms_preprocessed if term.startswith('#')]\n",
    "        count_hash.update(terms_hash)\n",
    "        # Considering sequences of two terms = bigrams\n",
    "        terms_bigram = bigrams(terms_all)\n",
    "        count_bigram.update(terms_bigram)\n",
    "        \n",
    "       \n",
    "    # Print the first 5 most frequent words\n",
    "    print('Most common terms:\\n', count_all.most_common(most_common))\n",
    "    print('\\n Most common terms, counted once per tweet:\\n', count_single.most_common(most_common))\n",
    "    print('\\n Most common hash-tag terms: ', count_hash.most_common(most_common))\n",
    "    print('\\n Most common bigrams: ', count_bigram.most_common(most_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('‚Äî', 55), ('Senate', 46), ('leaders', 45), ('#DoYourJob', 39), ('President', 32)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('‚Äî', 52), ('Senate', 46), ('leaders', 45), ('#DoYourJob', 39), ('President', 32)]\n",
      "\n",
      " Most common hash-tag terms:  [('#DoYourJob', 39), ('#ActOnClimate', 31), ('#Obamacare', 11), ('#GetCovered', 9), ('#ParisAgreement', 3)]\n",
      "\n",
      " Most common bigrams:  [(('Senate', 'leaders'), 43), (('President', 'Obama'), 29), (('Supreme', 'Court'), 23), (('Judge', 'Garland'), 18), (('Truth', 'Team'), 15)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('BarackObama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's do the same on the tweets from Donald Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('I', 54), ('great', 35), ('amp', 23), ('President', 23), ('years', 20)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('I', 47), ('great', 34), ('President', 23), ('amp', 21), ('years', 20)]\n",
      "\n",
      " Most common hash-tag terms:  [('#MAGA', 4), ('#MakeAmericaGreatAgain', 3), ('#GES2017', 2), ('#POTUSinAsia', 2), ('#PearlHarbor', 2)]\n",
      "\n",
      " Most common bigrams:  [(('U', 'S'), 11), (('Fake', 'News'), 9), (('Tax', 'Cuts'), 9), (('Crooked', 'Hillary'), 9), (('Stock', 'Market'), 7)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('realDonaldTrump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows already some interesting trends...  Out of curiosity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('Nous', 37), ('Je', 34), ('France', 30), ('Avec', 22), ('√©e', 20)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('Nous', 35), ('Je', 31), ('France', 28), ('Avec', 22), ('√©e', 19)]\n",
      "\n",
      " Most common hash-tag terms:  [('#CongresAMF', 8), ('#NeRienLaisserPasser', 7), ('#OnePlanet', 5), ('#JohnnyHallyday', 5), ('#SocialSummit17', 5)]\n",
      "\n",
      " Most common bigrams:  [(('Je', 'veux'), 10), (('ü•á', 'ü•á'), 9), (('apr', '√®s'), 8), (('aujourd', 'hui'), 7), (('lutte', 'contre'), 6)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('EmmanuelMacron', lang='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('@Space_Station', 62), ('@AstroKomrade', 35), ('space', 31), ('black', 28), ('us', 26)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('@Space_Station', 62), ('@AstroKomrade', 35), ('space', 30), ('black', 27), ('us', 26)]\n",
      "\n",
      " Most common hash-tag terms:  [('#BlackFriday', 16), ('#Cygnus', 9), ('#BlackHoleFriday', 8), ('#NASASiliconValley', 4), ('#ICYMI', 4)]\n",
      "\n",
      " Most common bigrams:  [(('black', 'holes'), 17), (('LIVE', 'NOW'), 12), (('pm', 'ET'), 10), (('black', 'hole'), 10), (('Join', 'us'), 9)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('NASA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
