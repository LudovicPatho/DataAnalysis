{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll go over the following topics:\n",
    "- Collecting data from Twitter\n",
    "- Text pre-processing using NLTK\n",
    "- Analysing term frequencies\n",
    "- Data Visualization\n",
    "- Sentiment Analysis\n",
    "\n",
    "This project was made by going through this (great) blog [post](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/).  Code extracts are mainly taken from there. I've also added pieces of code at various places following my interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to authorise our app to access Twitter on our behalf, we need to use the OAuth interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "import os\n",
    "consumer_key = os.environ['CONSUMER_KEY']\n",
    "consumer_secret = os.environ['CONSUMER_SECRET']\n",
    "access_token = os.environ['ACCESS_TOKEN']\n",
    "access_secret = os.environ['ACCESS_SECRET']\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display of the 10 tweets of the home timeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabie saoudite : les paris risqu√©s de Mohammed ben Salmane \n",
      "Le point de vue Francis Perrin, directeur de recherche‚Ä¶ https://t.co/RoiSUyEsQf\n",
      "RT @IRIS_SUP_: F√©licitation aux nouveaux dipl√¥m√©s IRIS Sup' 2017! üëè\n",
      "Pr√®s de 500 personnes √©taient pr√©sentes, dipl√¥m√©s, parents, amis, tous‚Ä¶\n",
      "RT @carole_gomez: Conf√©rence ce soir sur lutte contre la manipulation des comp√©titions en Fr. √† @InstitutIRIS avec T.Pujol, C.Kalb @ffhandb‚Ä¶\n",
      "RT @MouvEuropeen_Fr: Dans le cadre du #OnePlanetSummit le Mouvement Europ√©en et la @EIB vous invitent √† mettre en d√©bat les investissements‚Ä¶\n",
      "RT @InteragencyRAN: JUST OUT our new #report on #fragilestates by 2030 - looking at what the major #drivers of #state fragility are and how‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(5):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display of the JSON response :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Mon Dec 11 17:17:30 +0000 2017\", \"id\": 940269321837862912, \"id_str\": \"940269321837862912\", \"text\": \"Arabie saoudite : les paris risqu\\u00e9s de Mohammed ben Salmane \\nLe point de vue Francis Perrin, directeur de recherche\\u2026 https://t.co/RoiSUyEsQf\", \"truncated\": true, \"entities\": {\"hashtags\": [], \"symbols\": [], \"user_mentions\": [], \"urls\": [{\"url\": \"https://t.co/RoiSUyEsQf\", \"expanded_url\": \"https://twitter.com/i/web/status/940269321837862912\", \"display_url\": \"twitter.com/i/web/status/9\\u2026\", \"indices\": [117, 140]}]}, \"source\": \"<a href=\\\"http://twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web Client</a>\", \"in_reply_to_status_id\": null, \"in_reply_to_status_id_str\": null, \"in_reply_to_user_id\": null, \"in_reply_to_user_id_str\": null, \"in_reply_to_screen_name\": null, \"user\": {\"id\": 265897069, \"id_str\": \"265897069\", \"name\": \"IRIS\", \"screen_name\": \"InstitutIRIS\", \"location\": \"Paris\", \"description\": \"Acteur fran\\u00e7ais de la recherche strat\\u00e9gique et g\\u00e9opolitique. \n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "    \n",
    "for status in tweepy.Cursor(api.home_timeline).items(1):\n",
    "    # Process a single status\n",
    "    print(json.dumps(status._json)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing connections :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": 265897069, \"id_str\": \"265897069\", \"name\": \"IRIS\", \"screen_name\": \"InstitutIRIS\", \"location\": \"Paris\", \"description\": \"Acteur fran\\u00e7ais de la recherche strat\\u00e9gique et g\\u00e9opolitique. Ses activit\\u00e9s : la recherche, l\\u2019organisation de manifestations, la publication, la formation.\", \"url\": \"http://t.co/EctAbFIhS6\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/EctAbFIhS6\", \"expanded_url\": \"http://iris-france.org\", \"display_url\": \"iris-france.org\", \"indices\": [0, 22]}]}, \"description\": {\"urls\": []}}, \"protected\": false, \"followers_count\": 24428, \"friends_count\": 422, \"listed_count\": 603, \"created_at\": \"Mon Mar 14 09:29:08 +0000 2011\", \"favourites_count\": 670, \"utc_offset\": 3600, \"time_zone\": \"Paris\", \"geo_enabled\": true, \"verified\": false, \"statuses_count\": 9309, \"lang\": \"fr\", \"status\": {\"created_at\": \"Mon Dec 11 17:17:30 +0000 2017\", \"id\": 940269321837862912, \"id_str\": \"940269321837862912\", \"text\": \"Arabie saoudite : les paris risqu\\u00e9s de Mohammed ben Sa\n"
     ]
    }
   ],
   "source": [
    "for friend in tweepy.Cursor(api.friends).items(1):\n",
    "    print(json.dumps(friend._json)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing my own tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"¬´¬†Cool, bi√®res et bonbons gratuits¬†!¬†¬ª¬†: plong√©e dans une start-up d√©jant√©e\" #actualites #feedly https://t.co/cKWCAPZSVr\n",
      "Sun Jul 24 17:30:30 +0000 2016\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline).items():\n",
    "    #print(json.dumps(tweet._json))\n",
    "    print(tweet._json['text'])\n",
    "    print(tweet._json['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables :\n",
    "- `text`: the text of the tweet itself\n",
    "- `created_at`: the date of creation\n",
    "- `favorite_count`, `retweet_count`: the number of favourites and retweets\n",
    "- `favorited`, `retweeted`: boolean stating whether the authenticated user (you) have favourited or retweeted this tweet\n",
    "- `lang`: acronym for the language (e.g. ‚Äúen‚Äù for english)\n",
    "- `id`: the tweet identifier\n",
    "- `place`, `coordinates`, `geo`: geo-location information if available\n",
    "- `user`: the author‚Äôs full profile\n",
    "- `entities`: list of entities like URLs, @-mentions, hashtags and symbols\n",
    "- `in_reply_to_user_id`: user identifier if the tweet is a reply to a specific user\n",
    "- `in_reply_to_status_id`: status identifier id the tweet is a reply to a specific status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing tweets from another user :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ObamaFoundation: Watch: We hosted a Town Hall in New Delhi with @BarackObama and young leaders about how to drive change and make an im‚Ä¶\n",
      "Mon Dec 04 22:57:47 +0000 2017\n",
      "Michelle and I are delighted to congratulate Prince Harry and Meghan Markle on their engagement. We wish you a life‚Ä¶ https://t.co/KC9nmjZPuX\n",
      "Mon Nov 27 21:13:50 +0000 2017\n",
      "From the Obama family to yours, we wish you a Happy Thanksgiving full of joy and gratitude. https://t.co/xAvSQwjQkz\n",
      "Thu Nov 23 14:44:27 +0000 2017\n",
      "ME:  Joe, about halfway through the speech, I‚Äôm gonna wish you a happy birth--\n",
      "BIDEN:  IT‚ÄôS MY BIRTHDAY!\n",
      "ME:  Joe.‚Ä¶ https://t.co/5qLUsDoaMi\n",
      "Mon Nov 20 19:02:11 +0000 2017\n",
      "RT @ObamaFoundation: Today, we honor those who have honored our country with its highest form of service. https://t.co/IbJNCwIofL https://t‚Ä¶\n",
      "Sat Nov 11 15:13:46 +0000 2017\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(5):\n",
    "    #print(json.dumps(tweet._json))\n",
    "    print(tweet._json['text'])\n",
    "    print(tweet._json['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text tokenization using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'ObamaFoundation', ':', 'Watch', ':', 'We', 'hosted', 'a', 'Town', 'Hall', 'in', 'New', 'Delhi', 'with', '@', 'BarackObama', 'and', 'young', 'leaders', 'about', 'how', 'to', 'drive', 'change', 'and', 'make', 'an', 'im‚Ä¶']\n",
      "['Michelle', 'and', 'I', 'are', 'delighted', 'to', 'congratulate', 'Prince', 'Harry', 'and', 'Meghan', 'Markle', 'on', 'their', 'engagement', '.', 'We', 'wish', 'you', 'a', 'life‚Ä¶', 'https', ':', '//t.co/KC9nmjZPuX']\n",
      "['From', 'the', 'Obama', 'family', 'to', 'yours', ',', 'we', 'wish', 'you', 'a', 'Happy', 'Thanksgiving', 'full', 'of', 'joy', 'and', 'gratitude', '.', 'https', ':', '//t.co/xAvSQwjQkz']\n",
      "['ME', ':', 'Joe', ',', 'about', 'halfway', 'through', 'the', 'speech', ',', 'I', '‚Äô', 'm', 'gon', 'na', 'wish', 'you', 'a', 'happy', 'birth', '--', 'BIDEN', ':', 'IT', '‚Äô', 'S', 'MY', 'BIRTHDAY', '!', 'ME', ':', 'Joe.‚Ä¶', 'https', ':', '//t.co/5qLUsDoaMi']\n",
      "['RT', '@', 'ObamaFoundation', ':', 'Today', ',', 'we', 'honor', 'those', 'who', 'have', 'honored', 'our', 'country', 'with', 'its', 'highest', 'form', 'of', 'service', '.', 'https', ':', '//t.co/IbJNCwIofL', 'https', ':', '//t‚Ä¶']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(5):\n",
    "    tweet_text = tweet._json['text']\n",
    "    print(word_tokenize(tweet_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Enhancing the tokenization by accounting for @-mentions, emoticons, URLs and hash-tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@ObamaFoundation', ':', 'Watch', ':', 'We', 'hosted', 'a', 'Town', 'Hall', 'in', 'New', 'Delhi', 'with', '@BarackObama', 'and', 'young', 'leaders', 'about', 'how', 'to', 'drive', 'change', 'and', 'make', 'an', 'im', '‚Ä¶']\n",
      "['Michelle', 'and', 'I', 'are', 'delighted', 'to', 'congratulate', 'Prince', 'Harry', 'and', 'Meghan', 'Markle', 'on', 'their', 'engagement', '.', 'We', 'wish', 'you', 'a', 'life', '‚Ä¶', 'https://t.co/KC9nmjZPuX']\n",
      "['From', 'the', 'Obama', 'family', 'to', 'yours', ',', 'we', 'wish', 'you', 'a', 'Happy', 'Thanksgiving', 'full', 'of', 'joy', 'and', 'gratitude', '.', 'https://t.co/xAvSQwjQkz']\n",
      "['ME', ':', 'Joe', ',', 'about', 'halfway', 'through', 'the', 'speech', ',', 'I', '‚Äô', 'm', 'gonna', 'wish', 'you', 'a', 'happy', 'birth', '-', '-', 'BIDEN', ':', 'IT', '‚Äô', 'S', 'MY', 'BIRTHDAY', '!', 'ME', ':', 'Joe', '.', '‚Ä¶', 'https://t.co/5qLUsDoaMi']\n",
      "['RT', '@ObamaFoundation', ':', 'Today', ',', 'we', 'honor', 'those', 'who', 'have', 'honored', 'our', 'country', 'with', 'its', 'highest', 'form', 'of', 'service', '.', 'https://t.co/IbJNCwIofL', 'https://t', '‚Ä¶']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(5):\n",
    "    tweet_text = tweet._json['text']\n",
    "    print(preprocess(tweet_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the blog post, the tokeniser is probably far from perfect, but it gives you the general idea. The tokenisation is based on regular expressions (regexp), which is a common choice for this type of problem. Some particular types of tokens (e.g. phone numbers or chemical names) will not be captured, and will be probably broken into several tokens. To overcome this problem, as well as to improve the richness of your pre-processing pipeline, you can improve the regular expressions, or even employ more sophisticated techniques like Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the terms used in the last 200 tweets of Barack Obama :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-318154b77588>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtweet_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Create a list with all the terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mterms_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Update the counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcount_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterms_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "count_all = Counter()\n",
    "for tweet in tweepy.Cursor(api.user_timeline, id='BarackObama').items(200):\n",
    "    tweet_text = tweet._json['text']\n",
    "    # Create a list with all the terms\n",
    "    terms_all = [term for term in preprocess(tweet_text)]\n",
    "    # Update the counter\n",
    "    count_all.update(terms_all)\n",
    "    \n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most frequent words are not exactly meaningful.  Let's remove the common words, called \"stop-words\".  We can use NLTK for this.  We also include tweet-specific stop-words such as RT (used for re-tweets), \"via\" and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     d:\\Profiles\\cnozaradan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', 'RT', '‚Ä¶', '‚Äô', '‚Äú', 'The']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now adapt the code.  Let's embed it in a method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_stats(twitter_id, tweet_nbr=200, most_common=5, lang='english'):\n",
    "    punctuation = list(string.punctuation)\n",
    "    stop = stopwords.words(lang) + punctuation + ['rt', 'via', 'RT', '‚Ä¶', '‚Äô', '‚Äú', 'The','√©', 'les', 'a', 'La','Le','Il','√™tre']\n",
    "    count_all = Counter()\n",
    "    count_single = Counter()\n",
    "    count_hash = Counter()\n",
    "    for tweet in tweepy.Cursor(api.user_timeline, id=twitter_id).items(tweet_nbr):\n",
    "        tweet_text = tweet._json['text']\n",
    "        # Create a list with all the terms\n",
    "        terms_preprocessed = preprocess(tweet_text)\n",
    "        terms_all = [term for term in terms_preprocessed if term not in stop]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "        # Count terms only once, equivalent to Document Frequency\n",
    "        terms_single = set(terms_all)\n",
    "        count_single.update(terms_single)\n",
    "        # Count hashtags only\n",
    "        terms_hash = [term for term in terms_preprocessed if term.startswith('#')]\n",
    "        count_hash.update(terms_hash)\n",
    "       \n",
    "    # Print the first 5 most frequent words\n",
    "    print('Most common terms:\\n', count_all.most_common(most_common))\n",
    "    print('\\n Most common terms, counted once per tweet:\\n', count_single.most_common(most_common))\n",
    "    print('\\n Most common hash-tag terms: ', count_hash.most_common(most_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('‚Äî', 28), ('leaders', 25), ('Senate', 24), ('#DoYourJob', 23), ('Americans', 19)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('‚Äî', 28), ('leaders', 25), ('Senate', 24), ('#DoYourJob', 23), ('Americans', 19)]\n",
      "\n",
      " Most common hash-tag terms:  [('#DoYourJob', 23), ('#ActOnClimate', 18), ('#Obamacare', 10), ('#GetCovered', 9), ('#LeadOnLeave', 3)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('BarackObama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's do the same on the tweets from Donald Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('I', 32), ('great', 22), ('Tax', 15), ('years', 15), ('President', 14)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('I', 29), ('great', 21), ('years', 15), ('President', 14), ('Tax', 13)]\n",
      "\n",
      " Most common hash-tag terms:  [('#MAGA', 3), ('#MakeAmericaGreatAgain', 2), ('#GES2017', 2), ('#Periscope', 1), ('#WorldAIDSDay', 1)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('realDonaldTrump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows already some interesting trends...  Out of curiosity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms:\n",
      " [('Je', 27), ('Nous', 27), ('France', 21), ('jeunesse', 16), ('Afrique', 15)]\n",
      "\n",
      " Most common terms, counted once per tweet:\n",
      " [('Nous', 25), ('Je', 24), ('France', 20), ('jeunesse', 14), ('contre', 13)]\n",
      "\n",
      " Most common hash-tag terms:  [('#NeRienLaisserPasser', 7), ('#CongresAMF', 6), ('#OnePlanet', 5), ('#JohnnyHallyday', 5), ('#TraceMeetsMacron', 4)]\n"
     ]
    }
   ],
   "source": [
    "tweet_stats('EmmanuelMacron', lang='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
