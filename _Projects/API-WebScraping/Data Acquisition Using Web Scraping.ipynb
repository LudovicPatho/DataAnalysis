{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition Using Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://github.com/cnoza/DataAnalysis/blob/master/_Projects/README.md')\n",
    "content = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup to parse the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataAnalysis/README.md at master 路 cnoza/DataAnalysis 路 GitHub\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "title_text = parser.head.title.text\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataAnalysis/README.md at master 路 cnoza/DataAnalysis 路 GitHub\n"
     ]
    }
   ],
   "source": [
    "title_text = parser.head.find_all(\"title\")[0].text\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cnoza/DataAnalysis\n",
      "\n",
      "Preprocessing\n",
      "Machine Learning techniques\n",
      "Plotting techniques\n",
      "Statistics\n",
      "Networks\n",
      "Kaggle\n",
      "Data acquisition\n"
     ]
    }
   ],
   "source": [
    "h1 = parser.find_all('h1')\n",
    "for each in h1:\n",
    "    print(each.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: see Pima I, Pima II, 1984 US Congres votes, School budget\n",
      "Centering & scaling, standardization method: see Wine quality\n",
      "Sparse interactions: see School budget\n",
      "Standard scaler: see Wine\n",
      "Normalizer: see Stocks\n",
      "Pipelines with scikit-learn: see Pima I, 1984 US Congres votes, Wine quality, School budget\n",
      "Text tokenization using NLTK: see Twitter\n",
      "K-NN classifier with scikit-learn, supervised learning: see MNIST digits recognition, Pima I\n",
      "Regression with scikit-learn, supervised learning: see Boston housing, Gapminder I\n",
      "Lasso (L1) & Ridge (L2) regularized regression with scikit-learn, supervised learning: see Boston housing, Gapminder I\n",
      "Ridge (L2) regularized regression with scikit-learn, supervised learning: see Boston housing, Gapminder I, Auto\n",
      "Regularized regression using ElasticNet (scikit-learn): see Gapminder I\n",
      "Logistic regression with scikit-learn: see Pima I, Pima II, School budget\n",
      "Regression with categorical features: see Auto, Gapminder II\n",
      "ROC curve & AUC in scikit-learn: see Pima I\n",
      "Hyperparameter tuning with GridSearchCV and RandomizedSearchCV (scikit-learn): see Pima I, Gapminder I\n",
      "Decision trees with scikit-learn: see Pima I\n",
      "SVM (Super Vector Machines) with scikit-learn: see 1984 US Congres votes\n",
      "Hyperparameter tuning for the SVM classifier: see Wine quality\n",
      "NLP techniques (CountVectorizer, HashingVectorizer): see School budget\n",
      "Random Forest classifier: see School budget, Pima II\n",
      "Naive Bayes model: see Pima II\n",
      "KMeans clustering, cross-tabulation (unsupervized learning): see Grains, Fish, Stocks\n",
      "Hierarchical clustering (unsupervized learning): see Stocks\n",
      "Dimension reduction using PCA: see Grains, Fish\n",
      "Dimension reduction using TruncateSVD: see Wiki\n",
      "Dimension reduction using NMF: see Wiki, LCD digits\n",
      "Neural Networks using Keras: see Titanic, MNIST digits recognition\n",
      "Heatmap with seaborn: see Gapminder I\n",
      "Boxplot (pandas): see Gapminder II\n",
      "t-SNE visualization (unsupervized learning): see Grains, Stocks\n",
      "Basic EDA: see Lego shapes analysis (Datacamp), US Births, US gun deaths\n",
      "Manipulating pandas dataframes: see US firstnames trends (Datacamp)\n",
      "Network analysis using Networkx: see Networkx\n",
      "Titanic\n",
      "Using APIs: see Twitter, APIs\n",
      "Web Scraping: see Web Scraping\n"
     ]
    }
   ],
   "source": [
    "li = parser.select('#readme ul li')\n",
    "for each in li:\n",
    "    print(each.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
